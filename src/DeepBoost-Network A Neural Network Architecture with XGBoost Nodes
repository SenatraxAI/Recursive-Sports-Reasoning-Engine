I need to analyze this innovative idea before providing the implementation. Let me think through the architecture and implications.


# DeepBoost-Network: A Neural Network Architecture with XGBoost Nodes

## The Concept: Neural Networks Where Every Node is an XGBoost Model

Your idea represents an emerging paradigm in machine learning that some researchers call "Deep Forest" architectures or "Boosted Neural Networks." The fundamental insight is elegant: instead of having simple weighted sums and activation functions at each node in a neural network, you place an entire XGBoost model that learns a more complex, non-linear decision boundary. This creates a hybrid system that combines the ensemble power of gradient boosting with the hierarchical representation learning of neural networks.

The traditional neural network learns through backpropagation, adjusting weights based on gradient descent. XGBoost learns through sequential tree-building, where each new tree corrects the errors of the previous ensemble. When you combine these paradigms, you get something neither approach achieves alone: a system where each layer transforms the input into increasingly abstract representations, but that transformation itself is learned through the robust, regularization-heavy approach of gradient boosting.

For your Premier League prediction system, this architecture offers compelling advantages. Your input features already have rich hierarchical structure: raw player statistics feed into player value ratings, which feed into team strength calculations, which feed into match predictions. Each of these levels could be represented as a layer of XGBoost nodes, with each node learning to detect specific patterns in the data. One node might specialize in detecting when high-pressing teams face low-block defenses, another might specialize in home advantage interactions with team form, and another might focus on formation mismatches. The ensemble of these specialized XGBoost nodes creates a prediction that captures complex tactical interactions more effectively than a single XGBoost model or a standard neural network.

## Why This Architecture Makes Sense for Football Prediction

Football prediction has characteristics that make it particularly suitable for this hybrid approach. First, the signal-to-noise ratio is relatively low. Match outcomes depend heavily on random events (individual errors, refereeing decisions, post-shot expected goals that don't go in) that obscure the underlying team quality signal. XGBoost's built-in regularization and ability to handle small datasets well makes it robust for this domain. Second, the features have complex interactions that standard linear models miss. A player's value depends on their manager's tactical system, which depends on the opponent's formation, which depends on contextual factors like fatigue and motivation. Each XGBoost node in your network can learn to specialize in one of these interaction dimensions.

The layer-wise structure mirrors the natural hierarchy of football analysis. At the lowest level, you have raw player attributes: technical ability, physical attributes, tactical intelligence, and mental fortitude. These feed into the next level of player role ratings and system fit scores. Those feed into team-level metrics like tactical cohesion and formation balance. Those feed into match-level predictions. In a traditional XGBoost model, all these features are flattened and processed together, losing the hierarchical structure. In your XGBoost neural network, each layer can learn to transform features at its level while preserving the semantic meaning of the hierarchy.

The training approach also addresses a practical concern: data efficiency. Neural networks typically require large amounts of data to train effectively, but your historical Premier League dataset might only have a few thousand matches. XGBoost nodes are lighter weight and can learn meaningful patterns from smaller datasets. By stacking them in layers, you can achieve the representational power of a deep network without needing the massive data requirements that typically accompany deep learning.

## Architecture Design: The DeepBoost-Network Structure

The architecture consists of three main components working together. The XGBNeuron class represents a single node in the network, which is actually a complete XGBoost model trained to transform its input features into a higher-level representation. Each neuron takes the previous layer's output as input and produces class probability predictions as output. The CascadeLayer class groups multiple XGBNeuron instances together, representing one "hidden layer" in the network. Each layer takes the concatenated outputs from all neurons in the previous layer, creating a rich, multi-perspective transformation. The DeepBoostNetwork class manages the entire architecture, handling layer creation, training, early stopping, and prediction.

The key innovation is how data flows between layers. When data enters the first layer, it consists of your raw features: player ratings, manager profiles, formation encodings, and team strength metrics. Each XGBNeuron in the layer processes this input independently and produces probability outputs. These probability outputs are concatenated with the original features and passed to the next layer. This means each subsequent layer sees both the raw features and the learned transformations from the layer before, allowing it to correct errors and refine representations.

The training process uses a layer-wise greedy approach that differs fundamentally from backpropagation. You start with an empty network and add layers one at a time. For each new layer, you train all neurons in that layer using k-fold cross-validation on the current input. The cross-validation is crucial because it produces out-of-fold predictions that prevent data leakage between layers. After training a layer, you evaluate its contribution to prediction accuracy. If the new layer improves performance significantly, you keep it and try adding another. If improvement is marginal or negative, you stop growing the network. This approach is more stable than backpropagation for this hybrid architecture and leverages XGBoost's natural strength in sequential ensemble building.

## Complete Implementation

Here is the full implementation of the DeepBoost-Network architecture:

```python
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, log_loss
from xgboost import XGBClassifier, XGBRegressor
from copy import deepcopy
import warnings
warnings.filterwarnings('ignore')


@dataclass
class XGBNeuronConfig:
    """
    Configuration for a single XGBoost neuron/node in the network.
    """
    # Model parameters
    max_depth: int = 4
    learning_rate: int = 0.1
    n_estimators: int = 50
    min_child_weight: int = 3
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    gamma: float = 0.1
    reg_alpha: float = 0.1
    reg_lambda: float = 1.0
    
    # Diversity parameters - ensure neurons learn different patterns
    random_seed: int = 42
    feature_subsample_ratio: float = 0.7  # Use 70% of features per neuron
    
    # Output configuration
    output_dim: int = 2  # Number of classes for classification
    
    def create_neuron(self, seed_override: int = None) -> XGBClassifier:
        """Create a new XGBoost classifier with these parameters."""
        seed = seed_override if seed_override is not None else self.random_seed
        
        return XGBClassifier(
            max_depth=self.max_depth,
            learning_rate=self.learning_rate,
            n_estimators=self.n_estimators,
            min_child_weight=self.min_child_weight,
            subsample=self.subsample,
            colsample_bytree=self.colsample_bytree * self.feature_subsample_ratio,
            gamma=self.gamma,
            reg_alpha=self.reg_alpha,
            reg_lambda=self.reg_lambda,
            random_state=seed,
            use_label_encoder=False,
            eval_metric='logloss',
            verbosity=0
        )


@dataclass
class XGBNeuron:
    """
    A single neuron in the DeepBoost network.
    Wraps an XGBoost model and provides the transform interface.
    """
    config: XGBNeuronConfig
    model: Optional[XGBClassifier] = None
    neuron_id: int = 0
    oof_predictions: np.ndarray = field(default_factory=np.array)
    
    def fit(self, X: np.ndarray, y: np.ndarray, 
            n_splits: int = 5, neuron_idx: int = 0) -> 'XGBNeuron':
        """
        Train the neuron using k-fold cross-validation.
        
        This is crucial: we use out-of-fold predictions to prevent
        data leakage when this neuron becomes part of a larger network.
        """
        self.neuron_id = neuron_idx
        self.oof_predictions = np.zeros((len(y), self.config.output_dim))
        
        kfold = KFold(n_splits=n_splits, shuffle=True, 
                     random_state=self.config.random_seed + neuron_idx)
        
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X)):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # Create a fresh model for this fold
            model = self.config.create_neuron(
                seed_override=self.config.random_seed + neuron_idx * 100 + fold_idx
            )
            
            # Train the model
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
            
            # Store out-of-fold predictions for when this neuron
            # becomes part of a larger network
            self.oof_predictions[val_idx] = model.predict_proba(X_val)
        
        # Retrain on full data for use in final predictions
        self.model = self.config.create_neuron(
            seed_override=self.config.random_seed + neuron_idx * 1000
        )
        self.model.fit(X, y, verbose=False)
        
        return self
    
    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        Transform input features into higher-level representation.
        
        Returns probability predictions that capture what this neuron
        has learned about the input patterns.
        """
        if self.model is None:
            raise ValueError("Neuron must be fitted before transform")
        
        return self.model.predict_proba(X)
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Get class predictions from this neuron."""
        if self.model is None:
            raise ValueError("Neuron must be fitted before predict")
        
        return self.model.predict(X)
    
    def get_score(self, y: np.ndarray) -> Tuple[float, float]:
        """
        Calculate this neuron's contribution to prediction accuracy.
        Returns (accuracy, log_loss) based on out-of-fold predictions.
        """
        predictions = np.argmax(self.oof_predictions, axis=1)
        y_pred_proba = self.oof_predictions
        
        accuracy = accuracy_score(y, predictions)
        logloss = log_loss(y, y_pred_proba)
        
        return accuracy, logloss


@dataclass
class CascadeLayerConfig:
    """
    Configuration for a cascade layer (collection of XGBNeurons).
    """
    # Layer parameters
    n_neurons: int = 4  # Number of XGBoost nodes in this layer
    
    # Neuron configuration template
    neuron_config: XGBNeuronConfig = field(default_factory=XGBNeuronConfig)
    
    # Training parameters
    n_folds: int = 5
    min_accuracy_improvement: float = 0.001  # Minimum improvement to add this layer


@dataclass
class CascadeLayer:
    """
    Represents one hidden layer in the DeepBoost network.
    Contains multiple XGBNeuron instances that each learn different patterns.
    """
    config: CascadeLayerConfig
    layer_id: int = 0
    neurons: List[XGBNeuron] = field(default_factory=list)
    
    def fit(self, X: np.ndarray, y: np.ndarray, 
            previous_outputs: np.ndarray = None,
            layer_idx: int = 0) -> 'CascadeLayer':
        """
        Train all neurons in this layer.
        
        Each neuron sees the same input but learns different patterns
        due to random initialization and feature subsampling.
        """
        self.layer_id = layer_idx
        
        # Determine input features for this layer
        # If we have previous layer outputs, concatenate with raw features
        if previous_outputs is not None:
            layer_input = np.hstack([X, previous_outputs])
        else:
            layer_input = X
        
        self.neurons = []
        
        for neuron_idx in range(self.config.n_neurons):
            neuron_config = deepcopy(self.config.neuron_config)
            # Ensure each neuron has a different random seed for diversity
            neuron_config.random_seed += neuron_idx
            
            neuron = XGBNeuron(config=neuron_config)
            neuron.fit(layer_input, y, n_splits=self.config.n_folds, neuron_idx=neuron_idx)
            self.neurons.append(neuron)
        
        return self
    
    def transform(self, X: np.ndarray, 
                  previous_outputs: np.ndarray = None) -> np.ndarray:
        """
        Transform input through all neurons in the layer.
        
        Returns concatenated probability outputs from all neurons.
        """
        if previous_outputs is not None:
            layer_input = np.hstack([X, previous_outputs])
        else:
            layer_input = X
        
        # Get predictions from each neuron and concatenate
        neuron_outputs = [neuron.transform(layer_input) for neuron in self.neurons]
        
        return np.hstack(neuron_outputs)
    
    def get_layer_contribution(self, y: np.ndarray) -> Tuple[float, float]:
        """
        Calculate this layer's average contribution to predictions.
        """
        if not self.neurons:
            return 0.0, float('inf')
        
        # Average the out-of-fold predictions across all neurons
        all_predictions = np.array([n.oof_predictions for n in self.neurons])
        ensemble_predictions = np.mean(all_predictions, axis=0)
        
        accuracy = accuracy_score(y, np.argmax(ensemble_predictions, axis=1))
        logloss = log_loss(y, ensemble_predictions)
        
        return accuracy, logloss


@dataclass
class DeepBoostNetworkConfig:
    """
    Configuration for the complete DeepBoost network.
    """
    # Layer configurations
    layer_configs: List[CascadeLayerConfig] = field(default_factory=list)
    
    # Network growth parameters
    max_layers: int = 5
    min_accuracy_improvement: float = 0.002
    patience: int = 2  # Stop if no improvement for this many layers
    
    # Output parameters
    n_classes: int = 3  # home_win, draw, away_win
    output_activation: str = 'softmax'
    
    # Final classifier configuration
    final_classifier_config: Optional[XGBNeuronConfig] = None


class DeepBoostNetwork:
    """
    The main DeepBoost-Network architecture.
    
    A neural network where each node is an XGBoost model, trained
    using layer-wise greedy optimization.
    
    Usage:
        config = DeepBoostNetworkConfig()
        config.layer_configs = [
            CascadeLayerConfig(n_neurons=4),
            CascadeLayerConfig(n_neurons=6),
            CascadeLayerConfig(n_neurons=8)
        ]
        
        model = DeepBoostNetwork(config)
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
    """
    
    def __init__(self, config: DeepBoostNetworkConfig):
        self.config = config
        self.layers: List[CascadeLayer] = []
        self.final_classifier: Optional[XGBNeuron] = None
        
        # Training history
        self.layer_accuracies: List[float] = []
        self.layer_logloss: List[float] = []
        self.training_notes: List[str] = []
        
    def fit(self, X: np.ndarray, y: np.ndarray,
            X_val: np.ndarray = None, y_val: None) -> 'DeepBoostNetwork':
        """
        Build and train the network layer by layer.
        
        Uses greedy layer-wise training: add a layer, evaluate improvement,
        stop if no significant improvement.
        """
        # Convert labels to 0-indexed if needed
        if y.min() != 0:
            y = y - y.min()
        
        if X_val is not None and y_val is not None:
            if y_val.min() != 0:
                y_val = y_val - y_val.min()
        
        # Store training data for layer training
        self._X_train = X
        self._y_train = y
        
        # Calculate baseline accuracy with raw features only
        baseline_model = XGBClassifier(
            max_depth=3, n_estimators=50, random_state=42, verbosity=0
        )
        baseline_model.fit(X, y)
        baseline_pred = baseline_model.predict(X)
        baseline_accuracy = accuracy_score(y, baseline_pred)
        
        self.training_notes.append(f"Baseline XGBoost accuracy: {baseline_accuracy:.4f}")
        
        current_val_accuracy = baseline_accuracy
        no_improvement_count = 0
        
        # Build layers sequentially
        for layer_idx in range(self.config.max_layers):
            # Get configuration for this layer or use default
            if layer_idx < len(self.config.layer_configs):
                layer_config = self.config.layer_configs[layer_idx]
            else:
                layer_config = CascadeLayerConfig(
                    n_neurons=4 + layer_idx * 2,  # Grow neurons as we go deeper
                    neuron_config=XGBNeuronConfig(max_depth=3 + layer_idx)
                )
            
            # Get previous layer outputs (for stacking)
            previous_outputs = self._get_previous_layer_outputs(X)
            
            # Create and train new layer
            layer = CascadeLayer(config=layer_config)
            layer.fit(X, y, previous_outputs, layer_idx)
            
            # Evaluate this layer's contribution
            layer_accuracy, layer_logloss = layer.get_layer_contribution(y)
            
            self.training_notes.append(
                f"Layer {layer_idx + 1}: Accuracy={layer_accuracy:.4f}, "
                f"LogLoss={layer_logloss:.4f}, Neurons={len(layer.neurons)}"
            )
            
            # Check if this layer improves over the current best
            accuracy_improvement = layer_accuracy - current_val_accuracy
            
            if accuracy_improvement >= self.config.min_accuracy_improvement:
                # Significant improvement - keep this layer
                self.layers.append(layer)
                current_val_accuracy = layer_accuracy
                self.layer_accuracies.append(layer_accuracy)
                self.layer_logloss.append(layer_logloss)
                no_improvement_count = 0
                
                self.training_notes.append(
                    f"  -> Layer {layer_idx + 1} ACCEPTED "
                    f"(improvement: +{accuracy_improvement:.4f})"
                )
                
                # If validation data provided, check validation performance
                if X_val is not None and y_val is not None:
                    val_pred = self.predict(X_val)
                    val_accuracy = accuracy_score(y_val, val_pred)
                    self.training_notes.append(
                        f"  -> Validation accuracy: {val_accuracy:.4f}"
                    )
                    
            else:
                # No significant improvement - stop growing
                no_improvement_count += 1
                self.training_notes.append(
                    f"  -> Layer {layer_idx + 1} REJECTED "
                    f"(improvement: +{accuracy_improvement:.4f} < "
                    f"{self.config.min_accuracy_improvement})"
                )
                
                if no_improvement_count >= self.config.patience:
                    self.training_notes.append(
                        f"Early stopping at layer {layer_idx + 1} "
                        f"(patience={self.config.patience})"
                    )
                    break
        
        # Train final classifier on the full transformed representation
        self._train_final_classifier(X, y)
        
        self.training_notes.append(
            f"Training complete. Final network: {len(self.layers)} layers, "
            f"{sum(len(l.neurons) for l in self.layers)} total neurons"
        )
        
        return self
    
    def _get_previous_layer_outputs(self, X: np.ndarray) -> np.ndarray:
        """Get concatenated outputs from all previous layers."""
        if not self.layers:
            return None
        
        all_outputs = []
        current_input = X
        
        for layer in self.layers:
            layer_output = layer.transform(current_input)
            all_outputs.append(layer_output)
            current_input = np.hstack([X, np.hstack(all_outputs)])
        
        return np.hstack(all_outputs)
    
    def _train_final_classifier(self, X: np.ndarray, y: np.ndarray):
        """Train a final XGBoost classifier on the full network output."""
        # Get the full transformed representation
        previous_outputs = self._get_previous_layer_outputs(X)
        
        if previous_outputs is not None:
            full_input = np.hstack([X, previous_outputs])
        else:
            full_input = X
        
        # Create final classifier
        if self.config.final_classifier_config:
            final_config = self.config.final_classifier_config
            self.final_classifier = XGBNeuron(config=final_config)
        else:
            self.final_classifier = XGBNeuron(
                config=XGBNeuronConfig(
                    max_depth=3,
                    n_estimators=100,
                    output_dim=self.config.n_classes
                )
            )
        
        self.final_classifier.fit(full_input, y)
        
        # Store training accuracy
        final_pred = np.argmax(self.final_classifier.oof_predictions, axis=1)
        final_accuracy = accuracy_score(y, final_pred)
        self.training_notes.append(
            f"Final ensemble classifier accuracy: {final_accuracy:.4f}"
        )
    
    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        Transform input through the entire network.
        Returns the full transformed representation.
        """
        if not self.layers:
            return X
        
        previous_outputs = self._get_previous_layer_outputs(X)
        
        if previous_outputs is not None:
            return np.hstack([X, previous_outputs])
        else:
            return X
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Get probability predictions from the network.
        """
        if not self.layers and self.final_classifier is None:
            raise ValueError("Network must be fitted before prediction")
        
        # Get transformed representation
        transformed = self.transform(X)
        
        if self.final_classifier:
            return self.final_classifier.transform(transformed)
        else:
            # If no final classifier, average layer predictions
            current_input = X
            all_outputs = []
            
            for layer in self.layers:
                layer_output = layer.transform(current_input)
                all_outputs.append(layer_output)
                current_input = np.hstack([X, np.hstack(all_outputs)])
            
            return np.mean(all_outputs, axis=0)
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Get class predictions from the network.
        """
        proba = self.predict_proba(X)
        return np.argmax(proba, axis=1)
    
    def predict_with_confidence(self, X: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Get predictions along with confidence scores.
        """
        proba = self.predict_proba(X)
        max_proba = np.max(proba, axis=1)
        predictions = np.argmax(proba, axis=1)
        
        return {
            'predictions': predictions,
            'probabilities': proba,
            'confidence': max_proba
        }
    
    def get_layer_summary(self) -> pd.DataFrame:
        """
        Get a summary DataFrame of all layers in the network.
        """
        if not self.layer_accuracies:
            return pd.DataFrame()
        
        summary_data = []
        for i, (layer, acc, loss) in enumerate(
            zip(self.layers, self.layer_accuracies, self.layer_logloss)
        ):
            summary_data.append({
                'Layer': i + 1,
                'Neurons': len(layer.neurons),
                'Accuracy': f"{acc:.4f}",
                'LogLoss': f"{loss:.4f}",
                'Neuron_Depth': layer.config.neuron_config.max_depth
            })
        
        return pd.DataFrame(summary_data)
    
    def print_training_summary(self):
        """Print a formatted summary of the training process."""
        print("=" * 60)
        print("DeepBoost-Network Training Summary")
        print("=" * 60)
        
        for note in self.training_notes:
            print(note)
        
        print("\n" + "-" * 60)
        print("Layer Summary:")
        print("-" * 60)
        
        summary = self.get_layer_summary()
        if not summary.empty:
            print(summary.to_string(index=False))
        
        print("=" * 60)


# Integration with Premier League Prediction System

class PremierLeagueDeepBoostPredictor:
    """
    Integration layer that connects the DeepBoost-Network
    to your existing Premier League prediction system.
    """
    
    def __init__(self, fit_calculator: PlayerSystemFitCalculator):
        self.fit_calculator = fit_calculator
        self.model = None
        self.formation_encoder = self._create_formation_encoder()
        self.result_encoder = {'Home Win': 0, 'Draw': 1, 'Away Win': 2}
        self.result_decoder = {v: k for k, v in self.result_encoder.items()}
    
    def _create_formation_encoder(self) -> Dict[str, int]:
        """Encode formations as integers for the model."""
        formations = [
            '4-3-3', '4-2-3-1', '4-4-2', '3-5-2', 
            '3-4-3', '4-1-4-1', '4-3-2-1'
        ]
        return {f: i for i, f in enumerate(formations)}
    
    def prepare_match_features(
        self,
        home_team_data: Dict,
        away_team_data: Dict
    ) -> np.ndarray:
        """
        Prepare features for a single match prediction.
        
        Args:
            home_team_data: {
                'team_id': str,
                'player_ids': List[str],
                'formation': str,
                'manager_id': str,
                'manager_setup': ManagerTacticalSetup
            }
            away_team_data: Same structure for away team
        
        Returns:
            Feature vector for prediction
        """
        features = []
        
        # Home team features
        home_fit = self.fit_calculator.calculate_team_system_fit(
            Formation(home_team_data['formation']),
            home_team_data['player_ids'],
            home_team_data['manager_setup']
        )
        
        features.extend([
            home_fit['overall_fit'],
            home_fit['raw_average'],
            home_fit['squad_completeness'],
            len(home_fit['weak_links']),  # Number of weak positions
            self.formation_encoder.get(home_team_data['formation'], 0)
        ])
        
        # Away team features
        away_fit = self.fit_calculator.calculate_team_system_fit(
            Formation(away_team_data['formation']),
            away_team_data['player_ids'],
            away_team_data['manager_setup']
        )
        
        features.extend([
            away_fit['overall_fit'],
            away_fit['raw_average'],
            away_fit['squad_completeness'],
            len(away_fit['weak_links']),
            self.formation_encoder.get(away_team_data['formation'], 0)
        ])
        
        # Head-to-head features
        features.extend([
            home_fit['overall_fit'] - away_fit['overall_fit'],  # Relative fit
            home_fit['raw_average'] - away_fit['raw_average'],  # Relative talent
            1.0 if home_fit['overall_fit'] > away_fit['overall_fit'] else 0.0,  # Fit advantage
        ])
        
        # Manager tactical clash features
        home_manager = home_team_data['manager_setup']
        away_manager = away_team_data['manager_setup']
        
        features.extend([
            abs(home_manager.possession_preference - away_manager.possession_preference),
            abs(home_manager.pressing_intensity - away_manager.pressing_intensity),
            abs(home_manager.tempo_preference - away_manager.tempo_preference),
            home_manager.possession_preference - away_manager.possession_preference,
            home_manager.pressing_intensity - away_manager.pressing_intensity,
        ])
        
        return np.array(features).reshape(1, -1)
    
    def train(self, matches: List[Dict], labels: List[str]):
        """
        Train the DeepBoost-Network on historical matches.
        
        Args:
            matches: List of match data dictionaries
            labels: List of results ('Home Win', 'Draw', 'Away Win')
        """
        # Prepare feature matrix
        X = []
        for match in matches:
            home_data = match['home_team']
            away_data = match['away_team']
            
            features = self.prepare_match_features(home_data, away_data)
            X.append(features.flatten())
        
        X = np.array(X)
        y = np.array([self.result_encoder[l] for l in labels])
        
        # Create and configure the DeepBoost network
        config = DeepBoostNetworkConfig(
            layer_configs=[
                CascadeLayerConfig(
                    n_neurons=4,
                    neuron_config=XGBNeuronConfig(
                        max_depth=3,
                        n_estimators=50,
                        output_dim=3
                    )
                ),
                CascadeLayerConfig(
                    n_neurons=6,
                    neuron_config=XGBNeuronConfig(
                        max_depth=4,
                        n_estimators=60,
                        output_dim=3
                    )
                ),
                CascadeLayerConfig(
                    n_neurons=8,
                    neuron_config=XGBNeuronConfig(
                        max_depth=3,
                        n_estimators=70,
                        output_dim=3
                    )
                )
            ],
            max_layers=3,
            min_accuracy_improvement=0.005,
            patience=2,
            n_classes=3
        )
        
        self.model = DeepBoostNetwork(config)
        self.model.fit(X, y)
        self.model.print_training_summary()
    
    def predict_match(
        self,
        home_team_data: Dict,
        away_team_data: Dict
    ) -> Dict[str, any]:
        """
        Predict the outcome of a single match.
        """
        if self.model is None:
            raise ValueError("Model must be trained before prediction")
        
        features = self.prepare_match_features(home_team_data, away_team_data)
        
        result = self.model.predict_with_confidence(features)
        
        # Convert numeric prediction to label
        prediction_label = self.result_decoder[result['predictions'][0]]
        
        return {
            'prediction': prediction_label,
            'confidence': float(result['confidence'][0]),
            'probabilities': {
                'home_win': float(result['probabilities'][0][0]),
                'draw': float(result['probabilities'][0][1]),
                'away_win': float(result['probabilities'][0][2])
            },
            'recommendation': self._get_recommendation(result['probabilities'][0])
        }
    
    def _get_recommendation(self, probs: np.ndarray) -> str:
        """Generate a recommendation based on predicted probabilities."""
        max_prob = np.max(probs)
        
        if max_prob > 0.5:
            return f"Confident {self.result_decoder[np.argmax(probs)]}"
        elif max_prob > 0.4:
            return f"Lean {self.result_decoder[np.argmax(probs)]} (risky)"
        else:
            return "No strong recommendation - too uncertain"


# Example usage and demonstration

def demonstrate_deepboost_network():
    """
    Demonstrate the DeepBoost-Network with synthetic Premier League data.
    """
    from xgboost import XGBClassifier
    from sklearn.metrics import accuracy_score, classification_report
    
    # Generate synthetic Premier League-style data
    np.random.seed(42)
    n_samples = 1000
    
    # Generate features that simulate our system
    home_fit = np.random.beta(2, 1.5, n_samples)  # Home teams have slightly better fit
    away_fit = np.random.beta(1.8, 1.8, n_samples)
    home_talent = np.random.beta(2, 1.5, n_samples)
    away_talent = np.random.beta(1.8, 1.8, n_samples)
    
    formation_home = np.random.randint(0, 7, n_samples)
    formation_away = np.random.randint(0, 7, n_samples)
    
    possession_diff = home_fit - away_fit + np.random.normal(0, 0.1, n_samples)
    pressing_diff = home_fit - away_fit + np.random.normal(0, 0.1, n_samples)
    tempo_diff = home_talent - away_talent + np.random.normal(0, 0.1, n_samples)
    
    # Generate labels with realistic probabilities
    home_advantage = 0.1
    talent_diff = home_talent - away_talent
    fit_diff = home_fit - away_fit
    
    home_win_prob = 0.35 + home_advantage + 0.3 * talent_diff + 0.2 * fit_diff
    away_win_prob = 0.30 - home_advantage - 0.3 * talent_diff - 0.2 * fit_diff
    draw_prob = 1 - home_win_prob - away_win_prob
    
    # Normalize probabilities
    total = home_win_prob + away_win_prob + draw_prob
    home_win_prob /= total
    away_win_prob /= total
    draw_prob /= total
    
    # Sample outcomes
    outcomes = np.random.choice([0, 1, 2], n_samples, p=[home_win_prob.mean(), 
                                                          draw_prob.mean(), 
                                                          away_win_prob.mean()])
    
    # Create feature matrix
    X = np.column_stack([
        home_fit, away_fit, home_talent, away_talent,
        formation_home, formation_away,
        home_fit - away_fit, home_talent - away_talent,
        possession_diff, pressing_diff, tempo_diff
    ])
    
    y = outcomes
    
    # Split data
    split_idx = int(0.8 * n_samples)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print("=" * 60)
    print("DeepBoost-Network Demonstration")
    print("=" * 60)
    print(f"Training samples: {len(X_train)}")
    print(f"Test samples: {len(X_test)}")
    print(f"Number of features: {X.shape[1]}")
    print()
    
    # Train DeepBoost network
    config = DeepBoostNetworkConfig(
        layer_configs=[
            CascadeLayerConfig(
                n_neurons=4,
                neuron_config=XGBNeuronConfig(
                    max_depth=3,
                    n_estimators=50,
                    output_dim=3
                )
            ),
            CascadeLayerConfig(
                n_neurons=6,
                neuron_config=XGBNeuronConfig(
                    max_depth=4,
                    n_estimators=60,
                    output_dim=3
                )
            )
        ],
        max_layers=2,
        min_accuracy_improvement=0.005,
        patience=2,
        n_classes=3
    )
    
    model = DeepBoostNetwork(config)
    model.fit(X_train, y_train)
    model.print_training_summary()
    
    # Evaluate
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print("\n" + "=" * 60)
    print("Test Set Performance")
    print("=" * 60)
    print(f"DeepBoost-Network Accuracy: {accuracy:.4f}")
    print()
    
    # Compare with standard XGBoost
    baseline = XGBClassifier(max_depth=3, n_estimators=100, random_state=42, verbosity=0)
    baseline.fit(X_train, y_train)
    baseline_pred = baseline.predict(X_test)
    baseline_accuracy = accuracy_score(y_test, baseline_pred)
    
    print(f"Baseline XGBoost Accuracy: {baseline_accuracy:.4f}")
    print(f"Improvement: {(accuracy - baseline_accuracy):.4f}")
    
    print("\nClassification Report (DeepBoost-Network):")
    print(classification_report(y_test, y_pred, target_names=['Home Win', 'Draw', 'Away Win']))
    
    return model, accuracy, baseline_accuracy


if __name__ == "__main__":
    # Run demonstration
    model, db_accuracy, xgb_accuracy = demonstrate_deepboost_network()
```

## How This Integrates with Your Existing System

The DeepBoost-Network connects seamlessly to the player-system fit analysis you've already built. When you call `prepare_match_features` in the `PremierLeagueDeepBoostPredictor`, it uses your existing `PlayerSystemFitCalculator` to generate the tactical features that feed into the neural network. The network then learns how to combine these features in sophisticated ways that a single XGBoost model cannot.

The key insight is that each layer of XGBoost nodes can specialize in different aspects of match prediction. The first layer might learn basic patterns: home advantage effects, the relationship between team form and outcomes, and formation preferences. The second layer takes these transformed representations and learns higher-order interactions: how home advantage interacts with team form, how specific formation matchups create advantages, and how tactical fit differences translate to expected goals. Each subsequent layer refines the predictions further, correcting errors from earlier layers and capturing increasingly subtle patterns.

The layer-wise training approach also makes the system more interpretable. You can examine each layer's contribution to understand what patterns it's learning. The first layer typically captures obvious relationships, while deeper layers capture the complex interactions that distinguish accurate predictions from inaccurate ones. This transparency is valuable for debugging and for building trust in the model's predictions.

The architecture also scales naturally as you add more data or features. New features can be added to the input layer without retraining the entire network from scratch. You can also add more layers if the problem complexity warrants it, or add more neurons to existing layers for greater representational capacity. The modular design means you can experiment with different configurations without rebuilding the entire system.